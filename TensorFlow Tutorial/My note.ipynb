{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cd1992",
   "metadata": {},
   "source": [
    "# 1.0 Tensors basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec14f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97b47a",
   "metadata": {},
   "source": [
    "## Visual studio tips\n",
    "1. scalar (1 number), vector (1 dimensional), matrix (2 dimensional)\n",
    "2. Crtl+Shift+V to paste text from other sources\n",
    "3. Shift+Enter to display markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18cde4b",
   "metadata": {},
   "source": [
    "## Basic functions\n",
    "tf.ones(shape)\n",
    "tf.zeros(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03178027",
   "metadata": {},
   "source": [
    "## Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea2e4c",
   "metadata": {},
   "source": [
    "1. number of # represent order of heading\n",
    "2. $ equation $\n",
    "3. *italic*\n",
    "4. **bold**\n",
    "5. next line \\\n",
    "* bullets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eae0ba",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0527a",
   "metadata": {},
   "source": [
    "* A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes\n",
    "* Each tensor represents a partialy defined computation that will eventually produce a value\n",
    "* TensorFlow programs work by building a **graph** of Tensor objects that details how tensors are related. Running different parts of the **graph** allow results to be generated\n",
    "* Each tensor has a data **type** and a **shape**\n",
    "* `dtype`: \n",
    "  - `tf.string`: String variable (The b prefix is to indicate byte strings rather than unicode strings)    \n",
    "  - `tf.float32`: Float variable    \n",
    "  - `tf.int32`: Integer variable\n",
    "  - `tf.bool`: boolean, true or false\n",
    "* name: Optional, \"Const_1:0\" by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e99d3",
   "metadata": {},
   "source": [
    "### Types of tensors\n",
    "tf.constant(value, dtype, name = \"\")\\\n",
    "tf.Variable(value, dtype, name = \"\")\\\n",
    "tf.placeholder(value, dtype, name = \"\")\\\n",
    "tf.SparseTensor(value, dtype, name = \"\")\\\n",
    "- constance can't be changed while variables can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9709459",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = tf.constant([1,2,3], tf.int16) \n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 2D tensor\n",
    "matrix = [[1,2,3,4,5],\n",
    "          [6,7,8,9,10],\n",
    "          [11,12,13,14,15],\n",
    "          [16,17,18,19,20]]\n",
    "\n",
    "tensor = tf.Variable(matrix, dtype=tf.int32) \n",
    "print(tf.rank(tensor))\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d56c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor[0,2]) \n",
    "row1 = tensor[0]  # selects the first row\n",
    "print(row1)\n",
    "column1 = tensor[:, 0]  # selects the first column\n",
    "print(column1)\n",
    "row2and4 = tensor[1::2]  # selects second and fourth row\n",
    "print(row2and4)\n",
    "# a[start:end:step] means. a[1::2]=get every odd index, a[::2]=get every even index\n",
    "# a[2::2]=get every even starting at 2, a[2:4:2]=get every even starting at 2 and ending at 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6df35d",
   "metadata": {},
   "source": [
    "### Degree/rank of tensors\n",
    "the number of tensor dimension, 0=scalar,1=vector,2=matrix...\\\n",
    "tf.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d61c3",
   "metadata": {},
   "source": [
    "### Tensor shape\n",
    ".shape\\\n",
    "tf.shape()\\\n",
    "tf.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = tf.reshape(r2, [1,4])\n",
    "r4 = tf.reshape(r2, [4,-1])           # -1 tells the tensor to automatically calculate the size according to the first assigntment of 4\n",
    "print(r3)\n",
    "print(r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7b19c",
   "metadata": {},
   "source": [
    "### Evalualte tensor\n",
    "examine the value of tensor from time to time\\\n",
    "we need to run a *session* to evalue the tensors since they represent a partially complete computation\\\n",
    "the default graph holds all operations not speficied to any other graph. To evaluate the tensor stored in the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    r4.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d6fb3",
   "metadata": {},
   "source": [
    "# 2.0 Core Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adfc57",
   "metadata": {},
   "source": [
    "- Linear Regression\n",
    "- Classification\n",
    "- Clustering\n",
    "- Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe442a85",
   "metadata": {},
   "source": [
    "## 2.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4de307",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 2.5, 3, 4]\n",
    "y = [1, 4, 7, 9, 15]\n",
    "plt.plot(x, y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dafe24",
   "metadata": {},
   "source": [
    "### Direct call of np.polyfit()\n",
    "$ \\hat y = a+b*X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cf698",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))   # np.unique() removes duplicate elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee6d37",
   "metadata": {},
   "source": [
    "### Train the model to do linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466ebb2",
   "metadata": {},
   "source": [
    "#### Load a dataset\n",
    "required libraries: \n",
    "- numpy for multidimensional array calculation\n",
    "- pandas for dataset manipulating\n",
    "  - df.shape\n",
    "  - df.head(): show the first 5 items\n",
    "  - df.describe(): return statistics\n",
    "  - df.lable.hist() or df['lable'].hist()\n",
    "  - df.lable.value_counts() \n",
    "  - df.loc[*row_number*]\n",
    "- sklearn for 1)Preprocessing 2)Regression 3)Classification 4)Clustering 5)Model Selection 6)Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9aeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a520a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "\n",
    "import tensorflow.compat.v2.feature_column as fc\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "df_train = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
    "df_test = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
    "y_train = df_train.pop('survived')\n",
    "y_test = df_test.pop('survived')\n",
    "# pop() method removes the element at the specified position and stored it in y.\n",
    "# survive or not is the output of this titanic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.age.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sex.value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e2d06",
   "metadata": {},
   "source": [
    "#### Pre-process dataset\n",
    "Dataset comprises of categorical (text) and numerical (number) data. To train the model, categorical data can be converted to numbers via TensorFlow tools:\n",
    "1. convert panda dataframe to feature colunmns:\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list()\\\n",
    "    tf.feature_column.numeric_column()\n",
    "2. convert feature columns to a ```tf.data.Dataset``` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                       'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "  vocabulary = df_train[feature_name].unique()  # gets a list of all unique values from given feature column\n",
    "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "print(feature_columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input function defines how much data is split to batches and epochs to feed the training model\n",
    "# if data is too big might cause slow, difficult training and overfitting\n",
    "\n",
    "def make_input_fn(data_df, output_df, num_epochs=10, shuffle=True, batch_size=32): # num_epochs, shuffle, batch_size are default and can be changed when being called\n",
    "  def input_function():                                                            # inner function, this will be returned\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), output_df))            # create tf.data.Dataset object with data and its label\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(1000)                                                        # randomize order of data\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)                                   # split dataset into batches of 32 and repeat process for number of epochs\n",
    "    return ds                                                                      # return a batch of the dataset\n",
    "  return input_function                                                            # return a function object for use\n",
    "\n",
    "# here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "train_input_fn = make_input_fn(df_train, y_train)                                  # shuffle >> reorganize the order of the items  \n",
    "test_input_fn = make_input_fn(df_test, y_test, num_epochs=1, shuffle=False)        # we don't shuffle y as we don't train them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f56c4",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "`tf.estimator.LinearClassifier()`\\\n",
    "`model.train()`\\\n",
    "`model.evaluate()`\\\n",
    "`model.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and pass the feature columns we created earlier\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
    "\n",
    "# clears consoke output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "linear_est.train(train_input_fn) \n",
    "\n",
    "# Returns the loss value & metrics values for the model in test mode\n",
    "result = linear_est.evaluate(test_input_fn) \n",
    "\n",
    "clear_output() \n",
    "\n",
    "# statistics about our model. accuracy varies within [0,1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933c967",
   "metadata": {},
   "source": [
    "#### Prediction based on the trained model\n",
    "`model.predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates output predictions for the input samples & convert it to a list\n",
    "prediction = list(linear_est.predict(test_input_fn))\n",
    "clear_output() \n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd97b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'probabilities' indicate the survival chance  of the 4th passenger\n",
    "print(prediction[3]['probabilities'][0])\n",
    "# compare it with actual survival\n",
    "print(y_test.loc[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31086549",
   "metadata": {},
   "source": [
    "# 3.0 Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd63c16f",
   "metadata": {},
   "source": [
    "- NN takes inputs and maps they to outputs\n",
    "- ``Keras`` is introduced as a high-level neural networks API\n",
    "- $Y =activation[((\\sum_{i=0}^n w_i) x_i) + b]$\n",
    "- for every neuron in the next layer, there's only one bias from the previous layer\n",
    "- actiavation function returns a value between [0,1] for any vlalue of weighted sums + bias from the previous layer. \n",
    "  \n",
    "- Common activation functions: \n",
    "  - `Relu` (Rectified Linear Unit)\n",
    "  - `Tanh` (Hyperbolic Tangent)\n",
    "  - `Sigmoid`\n",
    "  - ...\n",
    "- Common types of data:\n",
    "  - Vector Data (2D)\n",
    "  - Timeseries or Sequence (3D)\n",
    "  - Image Data (4D)\n",
    "  - Video Data (5D)\n",
    "\n",
    "- **loss/cost functions**: For our training data we have the features (input) and the labels (expected output). The difference between the output from our network to the expected output is called loss functions. Common loss/cost functions include.\n",
    "  - Mean Squared Error (MSE, default)\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Hinge Loss (HL)\n",
    "\n",
    "- **Optimization function**: algorithm used to update weights and biases while backpropagation, to find the optimal paramaters (weights and biases) for our network. Common optimizers:\n",
    "  - Gradient Descent (GD)\n",
    "  - Stochastic Gradient Descent (SGD)\n",
    "  - Mini-Batch Gradient Descent\n",
    "  - Momentum\n",
    "  - Adam (momentum and GD combined)\n",
    "(https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6)\n",
    "\n",
    "GD determines which direction to go to get the global minimal, and the backpropagation process goes to that direction and updates weights and biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7f8d0d0",
   "metadata": {},
   "source": [
    "## Create a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e9e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "996504e6",
   "metadata": {},
   "source": [
    "a MNIST Fashion example: total 70000 images clothes are supposed to be classified into 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a3d3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist  \n",
    "\n",
    "# split into tetsing and training\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  \n",
    "\n",
    "print(train_images.shape)  # 60000 images of 28*28 pixels, varing in [0,255]\n",
    "print(test_images.shape)   # 10000 images of 28*28 pixels, varing in [0,255]\n",
    "print(train_labels.shape)  # 60000 values for clothe caterogies, varing in [0,9] \n",
    "print(test_labels.shape)   # 10000 values for clothe caterogies, varing in [0,9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pull out a random training image\n",
    "# plt.figure()\n",
    "# plt.imshow(train_images[1])\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "####### I don't know why but this is problematic. Don't run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6890e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563dc420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))\n",
    "\n",
    "# data normalization\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6a62f",
   "metadata": {},
   "source": [
    "## Create a NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a00aa45",
   "metadata": {},
   "source": [
    "model = `keras.Sequential()`:\\\n",
    "model.add()\\\n",
    "model.complie()\\\n",
    "model.evaluate()\\\n",
    "model.fit(x,y,batch_size, epochs): Trains the model for a fixed number of epochs\\\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381573ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.4950 - accuracy: 0.8273\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3681 - accuracy: 0.8672\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.3357 - accuracy: 0.8781\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.3120 - accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2925 - accuracy: 0.8916\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2803 - accuracy: 0.8958\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.2686 - accuracy: 0.8995\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.2578 - accuracy: 0.9041\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2489 - accuracy: 0.9079\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.2385 - accuracy: 0.9108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252ae983a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a 3-layer sequtial model\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # input layer \n",
    "    keras.layers.Dense(128, activation='relu'),  # hidden layer \n",
    "    keras.layers.Dense(10, activation='softmax') # output layer \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "641425b3",
   "metadata": {},
   "source": [
    "- flatten layer: 1D\n",
    "- Dense layers: all neurons in the previous layer connect to the every neurons in the next layer\n",
    "- `'softmax'`: common activation functinos for output layer to make sure all outputs of neurons add up to 1\n",
    "- An epoch is an iteration over the entire x and y data (forward and backward passes)\n",
    "- Hyperparameters are not trained in this case (but they can be trained if necessary):numbers of neuron, epoch, layer, and activation functions. Only weights and biases are trained during this 10 iterations\n",
    "\n",
    "\n",
    "`but why the total number of sample is 1875 instead of 6000?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3655c603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 15ms/step - loss: 0.3489 - accuracy: 0.8738\n",
      "Test accuracy: 0.8737999796867371\n"
     ]
    }
   ],
   "source": [
    "# The final accuracy of trained data is 0.9107\n",
    "# Now we want to evalueated the trained model on testing data\n",
    "# which is lower than the accuaracy of trained data >> overfitting\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=1) \n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "053afda7",
   "metadata": {},
   "source": [
    "*the fianl goals is to increase test accuracy instead of traning accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "030f160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Making predictions based on the test data\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1521fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.41598036e-07 1.79529974e-10 2.69420070e-10 1.87976842e-10\n",
      " 6.59264572e-07 8.01480317e-04 1.24198793e-08 6.21224474e-03\n",
      " 1.15325065e-07 9.92985249e-01]\n",
      "Ankle boot\n",
      "Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])\n",
    "print(class_names[np.argmax(predictions[0])])\n",
    "# np.argmax() returns the indices of the maximum values along an axis.\n",
    "print(class_names[test_labels[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6256fcd7",
   "metadata": {},
   "source": [
    "## Verifying Predictions\n",
    "I've written a small function here to help us verify predictions with some simple visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65472f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'] = COLOR\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\n",
    "\n",
    "def predict(model, image, correct_label):\n",
    "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "  prediction = model.predict(np.array([image]))\n",
    "  predicted_class = class_names[np.argmax(prediction)]\n",
    "\n",
    "  show_image(image, class_names[correct_label], predicted_class)\n",
    "\n",
    "\n",
    "def show_image(img, label, guess):\n",
    "  plt.figure()\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "  plt.title(\"Excpected: \" + label)\n",
    "  plt.xlabel(\"Guess: \" + guess)\n",
    "  plt.colorbar()\n",
    "  plt.grid(False)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def get_number():\n",
    "  while True:\n",
    "    num = input(\"Pick a number: \")\n",
    "    if num.isdigit():\n",
    "      num = int(num)\n",
    "      if 0 <= num <= 1000:\n",
    "        return int(num)\n",
    "    else:\n",
    "      print(\"Try again...\")\n",
    "\n",
    "num = get_number()\n",
    "image = test_images[num]\n",
    "label = test_labels[num]\n",
    "predict(model, image, label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aac49410",
   "metadata": {},
   "source": [
    "# 4.0 PANDAS basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef047f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f68e1a",
   "metadata": {},
   "source": [
    "# SA-PINN frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.sparse import diags\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "par = {\n",
    "    'k0rg': 0.7,\n",
    "    'k0rw': 1.,\n",
    "    'ng': 2.0,\n",
    "    'nw': 2.0,\n",
    "    'sgr': 0.,\n",
    "    'swr': 0.2,\n",
    "    'μg': 0.02,\n",
    "    'μw': 1.0,\n",
    "    'sgi': 0.,\n",
    "}\n",
    "\n",
    "def Krw(S, swr, sgr, k0rw, nw):\n",
    "    return k0rw * ((S - swr) / (1 - sgr - swr)) ** nw\n",
    "\n",
    "def Krw_p(S, p):\n",
    "    return Krw(S, p['swr'], p['sgr'], p['k0rw'], p['nw'])\n",
    "\n",
    "def Krg(S, swr, sgr, k0rg, ng):\n",
    "    return k0rg * ((1 - S - sgr) / (1 - sgr - swr)) ** ng\n",
    "\n",
    "def Krg_p(S, p):\n",
    "    return Krg(S, p['swr'], p['sgr'], p['k0rg'], p['ng'])\n",
    "\n",
    "def f(S, p):\n",
    "    return 1. / (1. + Krw_p(S, p) * p['μg'] / (Krg_p(S, p) * p['μw']))\n",
    "\n",
    "# Solve PDE\n",
    "dx = 0.01\n",
    "x_points = np.arange(0, 1.01, dx)\n",
    "t_points = np.linspace(0, 0.6, 61)\n",
    "\n",
    "# Discretize the PDE\n",
    "A = diags([1, -2, 1], [-1, 0, 1], shape=(len(x_points), len(x_points))).toarray()\n",
    "A[0, 0] = -1\n",
    "A[-1, -1] = -1\n",
    "A = A / dx**2\n",
    "\n",
    "def pde(t, S):\n",
    "    dSdt = A @ S\n",
    "    dSdt[0] = 1.0 - par['swr']\n",
    "    return dSdt\n",
    "\n",
    "# Initial condition\n",
    "S0 = np.full(len(x_points), par['sgi'])\n",
    "\n",
    "sol = solve_ivp(pde, (0, 0.6), S0, t_eval=t_points)\n",
    "\n",
    "# Plotting\n",
    "x_ticks = np.arange(0, 1.01, 0.01)\n",
    "ts = [0.12, 0.23, 0.35, 0.47, 0.59]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for t_ in ts:\n",
    "    t_idx = np.argmin(np.abs(sol.t - t_))\n",
    "    ax.plot(x_ticks, sol.y[:, t_idx], label=f't={t_}')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('S(x,t)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6cbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b6dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df346d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af5d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8701676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d74b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1144ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# or\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Dropout\n",
    "from tensorflow.keras import layers, activations\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9eb250",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b9098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b75c9b1afd1d46cffdb726aa8b6627fdb022e6a47d9da6b96d9625c5b7d0db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
